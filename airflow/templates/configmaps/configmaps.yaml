---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "airflow.fullname" . }}-env
  labels:
    chart: {{ .Chart.Name }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
data:
  TZ: Etc/UTC
  REDIS_PORT: '6379'
  POSTGRES_PORT: '5432'
  POSTGRES_DB: '{{ .Values.postgresql.EntrypointInitDB.projectDatabase }}'
  REDIS_HOST: '{{ .Release.Name }}-redis-master'
  FLOWER_PORT: '5555'
  AIRFLOW__CORE__DAGS_FOLDER: '{{ .Values.airflow.dagsPath }}'
  AIRFLOW__CORE__PLUGINS_FOLDER: '{{ .Values.airflow.pluginsPath }}'
  AIRFLOW__WEBSERVER__RBAC: "{{ .Values.airflow.rbac.enable }}"
  {{- range $setting, $option := .Values.airflow.env }}
  {{ $setting }}: '{{ $option }}'
  {{- end }}

{{- if and .Values.airflow.rbac.enable .Values.airflow.oauth.enable }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "airflow.fullname" . }}-webserver-rbac-config
  labels:
    chart: {{ .Chart.Name }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
data:
  webserver_config.py: |-
    import os
    from airflow import configuration as conf
    from flask_appbuilder.security.manager import AUTH_OAUTH

    basedir = os.path.abspath(os.path.dirname(__file__))

    # The SQLAlchemy connection string.
    SQLALCHEMY_DATABASE_URI = conf.get('core', 'SQL_ALCHEMY_CONN')

    # Flask-WTF flag for CSRF
    CSRF_ENABLED = True

    AUTH_TYPE = AUTH_OAUTH

    # Uncomment to setup Full admin role name
    # AUTH_ROLE_ADMIN = 'Admin'

    # Uncomment to setup Public role name, no authentication needed
    # AUTH_ROLE_PUBLIC = 'Public'

    # Will allow user self registration
    AUTH_USER_REGISTRATION = True

    # The default user self registration role
    AUTH_USER_REGISTRATION_ROLE = "Viewer"

    OAUTH_PROVIDERS = [{
        'name':'google',
        'whitelist': ['@' + conf.get('google', 'DOMAIN')],
        'token_key':'access_token',
        'icon':'fa-google',
            'remote_app': {
                'base_url':'https://www.googleapis.com/oauth2/v2/',
                'request_token_params':{
                    'scope': 'email profile'
                },
                'access_token_url':'https://accounts.google.com/o/oauth2/token',
                'authorize_url':'https://accounts.google.com/o/oauth2/auth',
                'request_token_url': None,
                'consumer_key': conf.get('google', 'CLIENT_ID'),
                'consumer_secret': conf.get('google', 'CLIENT_SECRET'),
            }
    }]

{{- end }}

{{- if .Values.airflow.metrics }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "airflow.fullname" . }}-metrics-script
  labels:
    chart: {{ .Chart.Name }}
    release: {{ .Release.Name }}
    heritage: {{ .Release.Service }}
data:
  prometheus_exporter.py: |-
    from datetime import timedelta
 
    from sqlalchemy import func
    from sqlalchemy import text
 
    from flask import Response
    from flask_admin import BaseView, expose
 
    # Views for Flask App Builder
    appbuilder_views = []
    try:
        from flask_appbuilder import BaseView as FABBaseView, expose as FABexpose
        class RBACMetrics(FABBaseView):
            route_base = "/admin/metrics/"
            @FABexpose('/')
            def list(self):
                return Response(generate_latest(), mimetype='text')
 
 
        # Metrics View for Flask app builder used in airflow with rbac enabled
        RBACmetricsView = {
            "view": RBACMetrics(),
            "name": "metrics",
            "category": "Prometheus exporter"
        }
        appbuilder_views = [RBACmetricsView]
 
    except ImportError:
        pass
 
 
    from airflow.plugins_manager import AirflowPlugin
    from airflow.settings import Session
    from airflow.models import DagStat, TaskInstance, DagModel, DagRun
    from airflow.utils.state import State
    from airflow.utils import timezone
    from airflow import configuration as conf
    from airflow import jobs
    #from celery.bin.control import control, inspect, status
 
    # Importing base classes that we need to derive
    from prometheus_client import generate_latest, REGISTRY
    from prometheus_client.core import GaugeMetricFamily
 
    from contextlib import contextmanager
 
    from celery import Celery
    import re
 
    app = Celery(broker=conf.get('celery', 'BROKER_URL'))
 
    @contextmanager
    def session_scope(session):
        """
        Provide a transactional scope around a series of operations.
        """
        try:
            yield session
            session.commit()
        except:
            session.rollback()
            raise
        finally:
            session.close()
 
    def get_dag_state_info():
        '''get dag info
        :return dag_info
        '''
        with session_scope(Session) as session:
            dag_status_query = session.query(
                DagStat.dag_id, DagStat.state, DagStat.count
            ).group_by(DagStat.dag_id, DagStat.state).subquery()
            return session.query(
                DagStat.dag_id, DagStat.state, DagStat.count,
                DagModel.owners
            ).join(DagModel, DagModel.dag_id == DagStat.dag_id).all()
 
 
    def get_task_state_info():
        '''get task info
        :return task_info
        '''
        with session_scope(Session) as session:
            task_status_query = session.query(
                TaskInstance.dag_id, TaskInstance.task_id,
                TaskInstance.state, TaskInstance.queue, func.count(TaskInstance.dag_id).label('value')
            ).group_by(TaskInstance.dag_id, TaskInstance.task_id, TaskInstance.state, TaskInstance.queue).subquery()
            return session.query(
                task_status_query.c.dag_id, task_status_query.c.task_id,
                task_status_query.c.state, task_status_query.c.queue, task_status_query.c.value, DagModel.owners
            ).join(DagModel, DagModel.dag_id == task_status_query.c.dag_id).all()
 
 
    def get_dag_duration_info():
        '''get duration of currently running DagRuns
        :return dag_info
        '''
        driver = Session.bind.driver
        durations = {
            'pysqlite': func.sum(
                (func.julianday(func.current_timestamp()) - func.julianday(DagRun.start_date)) * 86400.0
            ),
            'mysqldb': func.sum(func.timestampdiff(text('second'), DagRun.start_date, func.now())),
            'default': func.sum(func.now() - DagRun.start_date)
        }
        duration = durations.get(driver, durations['default'])
 
        with session_scope(Session) as session:
            return session.query(
                DagRun.dag_id,
                DagRun.run_id,
                duration.label('duration')
            ).group_by(
                DagRun.dag_id,
                DagRun.run_id
            ).filter(
                DagRun.state == State.RUNNING
            ).all()
 
 
    class MetricsCollector(object):
        '''collection of metrics for prometheus'''
 
        def describe(self):
            return []
 
        def collect(self):
            '''collect metrics'''
 
            # Task metrics
            task_info = get_task_state_info()
            t_state = GaugeMetricFamily(
                'airflow_task_status',
                'Shows the number of task starts with this status',
                labels=['dag_id', 'task_id', 'owner', 'status', 'queue']
            )

            # set all existing queues to 0 with state=exist
            workers_stats = app.control.inspect().stats()
            workers_queues = app.control.inspect().active_queues()
            queues_all = {}
            for worker in workers_stats:
                queue_name = workers_queues[worker][0]["name"]
                queues_all[queue_name] = 1
            
            for queue in queues_all:
                t_state.add_metric(['', 'exist', '', 'exist', queue], 0)

            for task in task_info:
                t_state.add_metric([task.dag_id, task.task_id, task.owners, task.state or 'none', task.queue], task.value)
            yield t_state
 
            # Dag Metrics
            dag_info = get_dag_state_info()
            d_state = GaugeMetricFamily(
                'airflow_dag_status',
                'Shows the number of dag starts with this status',
                labels=['dag_id', 'owner', 'status']
            )
            for dag in dag_info:
                d_state.add_metric([dag.dag_id, dag.owners, dag.state], dag.count)
            yield d_state
 
            # DagRun metrics
            dag_duration = GaugeMetricFamily(
                'airflow_dag_run_duration',
                'Duration of currently running dag_runs in seconds',
                labels=['dag_id', 'run_id']
            )
            driver = Session.bind.driver
            for dag in get_dag_duration_info():
                if driver == 'mysqldb' or driver == 'pysqlite':
                    dag_duration.add_metric([dag.dag_id, dag.run_id], dag.duration)
                else:
                    dag_duration.add_metric([dag.dag_id, dag.run_id], dag.duration.seconds)
            yield dag_duration
 
            BJ = jobs.BaseJob
            latest_scheduler_heartbeat = None
            metadatabase_health = 1
            scheduler_health_check_threshold = timedelta(seconds=30)
 
            try:
                with session_scope(Session) as session:
                    latest_scheduler_heartbeat = session.query(func.max(BJ.latest_heartbeat)).\
                        filter(BJ.state == 'running', BJ.job_type == 'SchedulerJob').\
                        scalar()
            except Exception:
                metadatabase_health = 0
 
            if not latest_scheduler_heartbeat:
                scheduler_status = 0
            else:
                if timezone.utcnow() - latest_scheduler_heartbeat <= scheduler_health_check_threshold:
                    scheduler_status = 1
                else:
                    scheduler_status = 0
 
            health = GaugeMetricFamily(
                'airflow_health',
                'Shows the number of task starts with this status',
                labels=['element']
            )
 
            health.add_metric(['metadatabase'], metadatabase_health)
            health.add_metric(['scheduler'], scheduler_status)
            yield health
 
 
            worker_max_concurrency = GaugeMetricFamily(
                'airflow_workers_concurrency',
                'short worker stats',
                labels=['host_name','queue']
            )
 
            worker_active_tasks = GaugeMetricFamily(
                'airflow_workers_active_tasks',
                'short worker stats',
                labels=['host_name','queue']
            )
 
            workers_stats = app.control.inspect().stats()
            workers_queues = app.control.inspect().active_queues()
            workers_active_tasks = app.control.inspect().active()
 
            for worker in workers_stats:
                max_concurrency = workers_stats[worker]["pool"]["max-concurrency"]
                queue_name = workers_queues[worker][0]["name"]
                count_active_tasks = len(workers_active_tasks[worker])
                host_name = re.sub('^\w+@', '', worker)
 
                worker_max_concurrency.add_metric([host_name,queue_name], max_concurrency)
                worker_active_tasks.add_metric([host_name,queue_name], count_active_tasks)
 
            yield worker_max_concurrency
            yield worker_active_tasks
 
 
 
 
 
    REGISTRY.register(MetricsCollector())
 
 
    class Metrics(BaseView):
        @expose('/')
        def index(self):
            return Response(generate_latest(), mimetype='text/plain')
 
 
    ADMIN_VIEW = Metrics(category="Prometheus exporter", name="metrics")
 
 
    class AirflowPrometheusPlugins(AirflowPlugin):
        '''plugin for show metrics'''
        name = "airflow_prometheus_plugin"
        operators = []
        hooks = []
        executors = []
        macros = []
        admin_views = [ADMIN_VIEW]
        flask_blueprints = []
        menu_links = []
        appbuilder_views = appbuilder_views
        appbuilder_menu_items = []
{{- end }}

{{- $root := . -}}
{{- range .Values.additionalResources.configMap2File }}
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ template "airflow.fullname" $root }}-{{ .name }}
  labels:
    chart: {{ $root.Chart.Name }}
    release: {{ $root.Release.Name }}
    heritage: {{ $root.Release.Service }}
data:
{{ toYaml .data | indent 2 }}
{{- end }}
